# Random Forests: Deep Dive

*Note*: These notes were generated by AI based on [lesson 2](https://www.youtube.com/watch?v=blyXCk4sgEg) of Introduction to Machine Learning for Coders by Jeremy Howard.

Over the next few lessons, we will delve deeply into random forests, examining their workings, troubleshooting, pros, cons, tuning parameters, and interpretation for data understanding.

## Review: `fastai` and Scikit-learn
`fastai` is a highly opinionated library incorporating state-of-the-art techniques into code. It often wraps or builds upon existing code, particularly from scikit-learn for structured data analysis. FastAI helps to get data into scikit-learn and interpret results from it.

**Environment Setup:**
* Notebooks reside in `FastAI/courses/ml1` and `FastAI/courses/dl1`.
* A symlink exists to the parent of the parent directory, `FastAI`, which contains modules.

**Using the FastAI library:**
1. Place notebooks/scripts in the same directory as `ml1` or `dl1` and import as usual.
2. Copy the `fastai` directory elsewhere.
3. Create a symlink to it from your working directory.

**Understanding the FastAI Repository Structure:**
* A GitHub repo called `fastai` exists.
* Inside this repo, there's a folder named `fastai` containing the actual FastAI library.
* `from fastai.imports import *` looks for `imports.py` inside the `fastai` folder.

**Symlinking:**
* Use `ln -s <source> <destination>` to create a symbolic link.
* If the destination is the current directory, it retains the original name.
* Symlinks function like aliases (Mac) or shortcuts (Windows).

**Importing without Symlinks (Discouraged):**
While `import sys` and appending a relative link can work, symlinking is the recommended approach for consistent imports.

## Data Preparation
We will use the Blue Book for Bulldozers competition data.

**Data Location:** `data/bulldozers`

**Reading the Data:**
Use `pd.read_csv` to read the CSV file, specifying date columns.

```python
import pandas as pd
df = pd.read_csv('data/bulldozers/Train.csv', parse_dates=['saledate'])
```

**Understanding the Evaluation Metric:**
The competition uses Root Mean Squared Log Error (RMSLE):

RMSLE = sqrt(mean((log(actual) - log(predicted))^2))

To optimize for RMSLE, replace `salePrice` with `log(salePrice)`:

```python
import numpy as np
df['salePrice'] = np.log(df['salePrice'])
```

Now, optimizing for Root Mean Squared Error (RMSE) is equivalent to optimizing for RMSLE.

**Converting Columns to Numbers:**
1. **Date Columns:** Remove the original date column and replace it with several new columns:
    * `is_quarter_start`
    * `is_year_end`
    * `elapsed` (days since January 1, 1970)
    * `year`
    * `month`
    * `dayofweek`
    * etc.

2. **Categorical Columns (Strings):** Use `train_cats` to convert strings to pandas categories.
    ```python
    def train_cats(df):
        for n,c in df.items():
            if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()
    ```

    ```python
    df['UsageBand'] = df['UsageBand'].astype('category')
    ```
    * These still *look* like strings, but their data type is now "category".
    * Use `.cat.categories` to view the possible categories.
    * Use `.cat.codes` to access the numerical codes.

3. **Numericalization:** Use `proc_df` to replace categorical columns with their `.cat.codes`.
    ```python
    def numericalize(df, col, name):
        if not is_numeric_dtype(col):
            df[name] = col.cat.codes+1
    ```

    ```python
    def proc_df(df, y_fld=None, skip_flds=None, do_scale=False, na_dict=None,
        preproc_fn=None, max_n_cat=None, subset=None, mapper=None):
        
        for n,c in df.items(): numericalize(df, c, n)
        if na_dict is None: na_dict = {}
        for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)
    ```

    * Each categorical column is replaced with a unique index based on the alphabetical order of feature names.
    * Continuous columns with missing values are replaced with the median, and an `_na` boolean column is added.

**Inspecting `numericalize` Source Code:**
Use `??numericalize` to view the source code. If the column is not numeric, replace it with `col.cat.codes + 1`. Adding `1` ensures that unknown values become `0` instead of `-1`.

## Initial Modelling and Evaluation
After numericalization, call `RandomForestRegressor.fit` and get the `.score`. A high R-squared value (e.g., 0.98) suggests a potentially overfit model.

## R-squared Explained
R-squared measures the proportion of variance explained by the model.

**Formula:**

R-squared = 1 - (SSres / SStot)

Where:
* **SStot:** Total sum of squares (variance of actual data from the mean). It represents the error of the simplest non-stupid model of predicting the mean.
* **SSres:** Residual sum of squares (variance of actual data from the model's predictions).

**Interpretation:**
* If the model is as effective as predicting the mean, SSres = SStot, and R-squared = 0.
* If the model is perfect, SSres = 0, and R-squared = 1.
* If the model is worse than the mean, R-squared becomes negative.

**Range of Values:**
R-squared can be any number less than 1. Negative R-squared values indicate a model worse than simply predicting the mean.

**Usefulness:**
R-squared provides a general sense of model fit across different models.

## Validation Sets and Overfitting
A high R-squared on the training set and a low R-squared on the validation set indicates overfitting.

**Importance of Validation Sets:**
Creating a representative validation set is the most crucial step in a machine learning project. It ensures that the model's performance on the validation set reflects its real-world performance.

**Time Series Data:**
If the data has a time component, ensure the validation set represents a future time period not included in the training set.

**Creating a Validation Set:**
Split the data into training and validation sets. For time series data, take the most recent data as the validation set.

```python
n_valid = 12000  # Size of the test set.
n_trn = len(df)-n_valid
X_train, X_valid = X[:n_trn].copy(), X[n_trn:].copy()
y_train, y_valid = y[:n_trn], y[n_trn:]

raw_train, raw_valid = raw[:n_trn].copy(), raw[n_trn:].copy()
```

**Validation vs. Test Sets:**
* **Validation Set:** Used for hyperparameter tuning and model selection.
* **Test Set:** A separate, final holdout set used *only once* at the end of the project to evaluate the final model's performance.
* To prevent overfitting, the test set should be removed from the data and only accessed when the final model is ready.

**Hyperparameters vs. Parameters:**
* **Parameters:** Learned from the data during training (e.g., the split points in a decision tree).
* **Hyperparameters:** Set *before* training and control the learning process (e.g., the maximum depth of a tree, the number of trees in a forest).

## Measuring Performance
Use RMSE to track performance.

```python
from sklearn.metrics import mean_squared_error

def rmse(x,y): return math.sqrt(((x-y)**2).mean())

def print_score(m):
    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),
                m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)
```

## Speeding Up Training
If training takes too long, consider reducing the dataset size.

**Subsampling:**
Use the `subset` parameter in `proc_df` to randomly sample a subset of the data.  Ensure the validation set remains consistent across different models.

```python
df_samp,y_samp,nas = proc_df(df, 'SalePrice', max_n_cat=max_n_cat, subset=30000)
X_samp = df_samp.drop('Id',axis=1)
```

## Single Decision Tree
Build a small, deterministic tree to understand the underlying process.

```python
m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)
m.fit(X_samp, y_samp)
print_score(m)
```

**Visualising the Tree:**
Decision trees consist of a sequence of binary splits.
Each node represents a decision based on a feature and value.

**Interpreting Node Values:**
* The top node shows the number of samples, the average `log(price)`, and the mean squared error if we simply predicted the average.
* Subsequent nodes show how the mean squared error improves after each split.

## Building a Tree from Scratch
The goal is to create a tree and to create the first binary decision. An algorithm is required. You can test all of the possible splits and see which one has a small RMSE.

1. For each variable, test the value as the first cut (split).
2. Find a variable to split into such that the two groups are as different to each other as possible.
3. Identify which variable and which split point achieves that result.
4. Compute and find what the root mean squared error is.

To determine which split is better, use a weighted average of the mean squared errors of the two groups the split creates. The weights are based on the size of each group.

## Random Forests
1. To create a forest, use a statistical technique called bagging.
2. Create five different models.
3. Average the five models.

**Bagging (Bootstrap Aggregating):**
Create multiple deep, overfit trees trained on random subsets of the data. Each tree is trained on a different random sample. The average of these trees reduces overfitting and improves generalization.

1. Randomly sample rows from the data.
2. Build a deep tree.
3. Make predictions by running test data through the tree.
4. Take the average values from leaf nodes.
5. Average all leaf nodes together.

```python
m = RandomForestRegressor(n_jobs=-1)
m.fit(X_train, y_train)
print_score(m)
```

**Understanding the Random Forest's Ensemble Nature:**
Access individual trees using `model.estimators_`. A tree is a method of picking out the data points that look most similar to the one you are looking at.
* Each estimator makes a separate prediction.
* The final prediction is the average of all individual tree predictions.

The original Random Forest paper highlights that low correlation between trees is crucial for model performance. Research shows that reducing tree correlation is often more important than improving the accuracy of individual trees. As a result, models like `ExtraTreesRegressor`, which select random subsets of features for each split, can achieve strong overall results even if their individual trees are weak predictors.

## Tuning Hyperparameters
After building a random forest, the goal is to create predictive models that have errors that are as much as possible not correlated with each other.
1. Find out what has a good validation set.
2. Find out the best approach for RMSE, or not.

**Number of Estimators (`n_estimators`):**
Increasing the number of trees generally improves performance, but there are diminishing returns. To determine how many models should be built, average them together.

```python
preds = np.stack([t.predict(X_valid) for t in m.estimators_])
np.mean(preds[:,0]), y_valid[0]
```

The number of trees can be set using the `n_estimators` parameter.

**Out-of-Bag (OOB) Score:**
To train the model with one set of data, find the rows that did not get used. Pass those rows through the first tree. To create a validation set for the second tree, pass the rows that were not used to train for that tree.

* Calculate OOB predictions by averaging the trees that *didn't* use the row for training.
* `oob_score=True` calculates and stores the OOB score in `m.oob_score_`.
* Generally underestimates generalisability but is useful for hyperparameter tuning, especially with grid search.

```python
m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3,
                          max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)
```

**Sampling Without Replacement:**
Use sampling with replacement to get approximately 63% of the rows represented. 

The `set_rf_samples()` function in `fastai` fits each tree on a different subset of training data, speeding up computation and aiding hyperparameter tuning. However, it is incompatible with `oob_score`, so avoid using out-of-bag scoring when sampling rows this way.

**Minimum Samples per Leaf (`min_samples_leaf`):**
Controls the minimum number of samples required in a leaf node. Higher values can prevent overfitting. A sample of values would be 1, 3, 5, 10, or 25.

**Max Features (`max_features`):**
* Specifies the maximum number of features to consider at each split.
* Decreasing this value can reduce correlation between trees.
* Common values: `0.5`, `sqrt`, `log2`.

## Other Considerations
**Extremely Randomized Trees:**
An alternative approach is to try just a few splits of a few variables to train the model with more randomness. It has more randomness but with more time, it can build more trees to get better generalization.

**Feature Importance:**
We will cover feature importance and model interpretation in the next lesson.

## Summary
* Random forests are a powerful and versatile machine learning technique.
* Proper data preparation and validation are crucial for building effective models.
* Hyperparameter tuning can significantly improve performance.
* The out-of-bag score provides a useful estimate of generalisability.
